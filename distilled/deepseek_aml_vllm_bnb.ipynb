{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796cf06d-e4d3-422e-8301-a961e9520f52",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DeepSeek R1 Distillation 1.5B vLLM serving using the Azure ML Python SDK\n",
    "\n",
    "> [1] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.<br>[2] Please make sure you prepare [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7133f65-76f4-49ac-a160-5acb9a9c9464",
   "metadata": {},
   "source": [
    "You may need to install `azure-ai-ml` and `azure-identity` before start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4ab4eb-5810-4fec-9906-3660855554e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -q azure-ai-ml azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb139b1d-500c-4ef2-9af3-728f2a5ea05f",
   "metadata": {},
   "source": [
    "## 1. Prepare Environment Parameters\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5234c47-b3e5-4218-8a98-3988c8991643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 15:25:00,033 - logger - DEBUG - ===== 0. Azure ML Deployment Info =====\n",
      "2025-03-05 15:25:00,034 - logger - DEBUG - AZURE_SUBSCRIPTION_ID=e56790f8-0506-49eb-95b8-82817828d59d\n",
      "2025-03-05 15:25:00,035 - logger - DEBUG - AZURE_RESOURCE_GROUP=dev\n",
      "2025-03-05 15:25:00,035 - logger - DEBUG - AZURE_WORKSPACE=test-vllm\n",
      "2025-03-05 15:25:00,036 - logger - DEBUG - HF_MODEL_NAME_OR_PATH=deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
      "2025-03-05 15:25:00,037 - logger - DEBUG - azure_env_name=ds-llama-70b-env-bnb\n",
      "2025-03-05 15:25:00,037 - logger - DEBUG - azure_model_name=ds-llama-70b-bnb-model\n",
      "2025-03-05 15:25:00,038 - logger - DEBUG - azure_endpoint_name=ds-llama-70b-bnb-ep\n",
      "2025-03-05 15:25:00,038 - logger - DEBUG - azure_deployment_name=blue\n",
      "2025-03-05 15:25:00,039 - logger - DEBUG - azure_serving_cluster_size=Standard_ND40rs_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from logger import logger\n",
    "from datetime import datetime\n",
    "\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "HF_MODEL_NAME_OR_PATH = 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B'\n",
    "\n",
    "with open(\"config.yml\") as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "AZURE_SUBSCRIPTION_ID = d[\"config\"][\"AZURE_SUBSCRIPTION_ID\"]\n",
    "AZURE_RESOURCE_GROUP = d[\"config\"][\"AZURE_RESOURCE_GROUP\"]\n",
    "AZURE_WORKSPACE = d[\"config\"][\"AZURE_WORKSPACE\"]\n",
    "HF_TOKEN = d[\"config\"][\"HF_TOKEN\"]\n",
    "IS_DEBUG = d[\"config\"][\"IS_DEBUG\"]\n",
    "\n",
    "azure_env_name = 'ds-llama-70b-env-bnb'\n",
    "azure_model_name = 'ds-llama-70b-bnb-model'\n",
    "azure_endpoint_name = 'ds-llama-70b-bnb-ep'\n",
    "azure_deployment_name = 'blue'\n",
    "azure_serving_cluster_size = 'Standard_ND40rs_v2'\n",
    "\n",
    "\n",
    "if IS_DEBUG:\n",
    "    logger.debug(\"===== 0. Azure ML Deployment Info =====\")\n",
    "    logger.debug(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "    logger.debug(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "    logger.debug(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "    logger.debug(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "\n",
    "    logger.debug(f\"azure_env_name={azure_env_name}\")\n",
    "    logger.debug(f\"azure_model_name={azure_model_name}\")\n",
    "    logger.debug(f\"azure_endpoint_name={azure_endpoint_name}\")\n",
    "    logger.debug(f\"azure_deployment_name={azure_deployment_name}\")\n",
    "    logger.debug(f\"azure_serving_cluster_size={azure_serving_cluster_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9843e0f-3cf1-4e86-abb7-a49919fac8d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Serving preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb4a273-ba31-4f47-a2fd-dc8cdea390f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 15:25:02,481 - logger - INFO - ===== 2. Serving preparation =====\n",
      "2025-03-05 15:25:02,482 - logger - INFO - Calling DefaultAzureCredential.\n",
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "logger.info(f\"===== 2. Serving preparation =====\")\n",
    "logger.info(f\"Calling DefaultAzureCredential.\")\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    ml_client = MLClient(\n",
    "        credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a377b-d10c-413e-a67b-2c11a3cff7fd",
   "metadata": {},
   "source": [
    "### 2.2. Create model asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73532c39-3fdd-40a7-b2be-9f5a2f22443a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    model_name,\n",
    "    job_name=None,\n",
    "    model_dir=\"outputs\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    "):\n",
    "    try:\n",
    "        latest_model_version = max(\n",
    "            [int(m.version) for m in ml_client.models.list(name=model_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Model asset, but will update the Model.\")\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(\n",
    "                name=model_name, version=latest_model_version\n",
    "            )\n",
    "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        run_model = Model(\n",
    "            name=model_name,\n",
    "            path=model_dir,\n",
    "            description=\"Model created from run.\",\n",
    "            type=model_type,  # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        logger.info(f\"Created Model asset: {model_name} from {model_dir}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aaa9671-fc98-4e5e-a70c-7771caa1c7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 15:25:04,020 - logger - INFO - Exception: (UserError) The specified resource was not found.\n",
      "Code: UserError\n",
      "Message: The specified resource was not found.\n",
      "Exception Details:\t(ModelNotFound) Model container with name: ds-llama-70b-bnb-model not found.\n",
      "\tCode: ModelNotFound\n",
      "\tMessage: Model container with name: ds-llama-70b-bnb-model not found.\n",
      "2025-03-05 15:25:09,375 - logger - INFO - Created Model asset: ds-llama-70b-bnb-model from deepseek-adapter\n"
     ]
    }
   ],
   "source": [
    "model = get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    azure_model_name,\n",
    "    job_name=None,\n",
    "    model_dir=\"deepseek-adapter\", # dummy model file folder\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac357a-3944-4702-a338-b9f4b67dadc9",
   "metadata": {},
   "source": [
    "#### Docker environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b19bc6-dc43-4948-8c71-97a033a5f5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 15:25:09,984 - logger - INFO - Exception: (UserError) System.Net.Http.HttpConnectionResponseContent\n",
      "Code: UserError\n",
      "Message: System.Net.Http.HttpConnectionResponseContent\n",
      "\u001b[32mUploading docker (0.0 MBs): 100%|██████████| 350/350 [00:00<00:00, 11657.41it/s]\n",
      "\u001b[39m\n",
      "\n",
      "2025-03-05 15:25:17,965 - logger - INFO - Created Environment asset: ds-llama-70b-env-bnb\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "\n",
    "\n",
    "def get_or_create_docker_environment_asset(\n",
    "    ml_client, env_name, docker_dir, inference_config=None, update=False\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_env_version = max(\n",
    "            [int(e.version) for e in ml_client.environments.list(name=env_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\n",
    "                \"Found Environment asset, but will update the Environment.\"\n",
    "            )\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(\n",
    "                name=env_name, version=latest_env_version\n",
    "            )\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            build=BuildContext(path=docker_dir),\n",
    "            name=env_name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "            inference_config=inference_config,\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        logger.info(f\"Created Environment asset: {env_name}\")\n",
    "\n",
    "    return env_asset\n",
    "\n",
    "\n",
    "inference_config = {\n",
    "    \"liveness_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/health\",\n",
    "    },\n",
    "    \"readiness_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/health\",\n",
    "    },\n",
    "    \"scoring_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/\",\n",
    "    },\n",
    "}\n",
    "\n",
    "env = get_or_create_docker_environment_asset(\n",
    "    ml_client, azure_env_name, \"docker\", inference_config, update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb187e-370f-4481-9d82-a38ae982c1e3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serving\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Create endpoint\n",
    "\n",
    "Create an endpoint. This process does not provision a GPU cluster yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c22433-4ab8-4db9-956b-7f437b86dfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 15:25:17,974 - logger - INFO - ===== 3. Serving =====\n",
      "2025-03-05 15:26:22,362 - logger - INFO - \n",
      "---Endpoint created successfully---\n",
      "\n",
      "2025-03-05 15:26:22,377 - logger - INFO - Creating Endpoint took 1 minute and 4.39 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.1 ms, sys: 7.98 ms, total: 104 ms\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    IdentityConfiguration,\n",
    "    ManagedIdentityConfiguration,\n",
    ")\n",
    "\n",
    "logger.info(f\"===== 3. Serving =====\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Check if the endpoint already exists in the workspace\n",
    "try:\n",
    "    endpoint = ml_client.online_endpoints.get(azure_endpoint_name)\n",
    "    logger.info(\"---Endpoint already exists---\")\n",
    "except:\n",
    "    # Create an online endpoint if it doesn't exist\n",
    "\n",
    "    # Define the endpoint\n",
    "    endpoint = ManagedOnlineEndpoint(\n",
    "        name=azure_endpoint_name,\n",
    "        description=f\"Test endpoint for {model.name}\",\n",
    "    )\n",
    "\n",
    "# Trigger the endpoint creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(endpoint).wait()\n",
    "    logger.info(\"\\n---Endpoint created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(f\"Endpoint creation failed. Detailed Response:\\n{err}\") from err\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "from humanfriendly import format_timespan\n",
    "\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating Endpoint took {timespan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a05af-0aca-4d36-9c0f-bfa4dcc6203b",
   "metadata": {},
   "source": [
    "### 3.2. Create Deployment\n",
    "\n",
    "Create a Deployment. This takes a lot of time as GPU clusters must be provisioned and the serving environment must be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d08ab32-5cd6-439f-bb34-13ddc200a279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"MODEL_NAME\": HF_MODEL_NAME_OR_PATH,\n",
    "    \"VLLM_ARGS\": \"--tensor-parallel-size 8 --max-model-len 32768 --enforce-eager --quantization bitsandbytes --load-format bitsandbytes --dtype float16\",\n",
    "    \"HUGGING_FACE_HUB_TOKEN\": HF_TOKEN,\n",
    "}\n",
    "deployment_env_vars = {**env_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3afaa8b-5af1-49d1-990f-414da0effe8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint ds-llama-70b-bnb-ep exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 16:01:40,438 - logger - INFO - \n",
      "---Deployment created successfully---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 s, sys: 289 ms, total: 4.43 s\n",
      "Wall time: 28min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from azure.ai.ml.entities import (    \n",
    "    OnlineRequestSettings,\n",
    "    CodeConfiguration,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    Environment\n",
    ")\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=azure_deployment_name,\n",
    "    endpoint_name=azure_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=azure_serving_cluster_size,\n",
    "    instance_count=1,\n",
    "    environment_variables=deployment_env_vars,    \n",
    "    environment=env,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=2,\n",
    "        request_timeout_ms=120000, \n",
    "        max_queue_wait_ms=240000\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=5,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=30,\n",
    "        initial_delay=120\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        timeout=2,\n",
    "        period=10,\n",
    "        initial_delay=120,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Trigger the deployment creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(deployment).wait()\n",
    "    logger.info(\"\\n---Deployment created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err\n",
    "    \n",
    "endpoint.traffic = {azure_deployment_name: 100}\n",
    "endpoint_poller = ml_client.online_endpoints.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f7dfe9-e27e-449d-a5cb-425663651c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_results = endpoint_poller.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ae58e5f-999f-449e-b5ac-c766c184715f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_mode: key\n",
      "description: Test endpoint for ds-llama-70b-bnb-model\n",
      "id: /subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/resourceGroups/dev/providers/Microsoft.MachineLearningServices/workspaces/test-vllm/onlineEndpoints/ds-llama-70b-bnb-ep\n",
      "identity:\n",
      "  principal_id: b88b1426-4b8c-4349-9b7c-09742b4a5e7c\n",
      "  tenant_id: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "  type: system_assigned\n",
      "kind: Managed\n",
      "location: westeurope\n",
      "mirror_traffic: {}\n",
      "name: ds-llama-70b-bnb-ep\n",
      "openapi_uri: https://ds-llama-70b-bnb-ep.westeurope.inference.ml.azure.com/swagger.json\n",
      "properties:\n",
      "  AzureAsyncOperationUri: https://management.azure.com/subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/providers/Microsoft.MachineLearningServices/locations/westeurope/mfeOperationsStatus/oeidp:10331bed-c16d-4680-834e-e054c4eeaeb9:cbb8b891-c853-4ec3-bbdf-bf192102eb99?api-version=2022-02-01-preview\n",
      "  azureml.onlineendpointid: /subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/resourcegroups/dev/providers/microsoft.machinelearningservices/workspaces/test-vllm/onlineendpoints/ds-llama-70b-bnb-ep\n",
      "  createdAt: 2025-03-05T15:25:21.429905+0000\n",
      "  createdBy: Jihua Liu\n",
      "  lastModifiedAt: 2025-03-05T16:01:41.950913+0000\n",
      "provisioning_state: Succeeded\n",
      "public_network_access: enabled\n",
      "scoring_uri: https://ds-llama-70b-bnb-ep.westeurope.inference.ml.azure.com/\n",
      "tags: {}\n",
      "traffic:\n",
      "  blue: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(endpoint_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de81f846-58e0-4717-92ac-9871787e7a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = endpoint_results.name\n",
    "keys = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "primary_key = keys.primary_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f437f6-153a-42d5-ab22-0011d0fe2481",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Test\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b67088d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create your prompt\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant that helps customers find information. As an assistant, you respond to questions in a concise and unique manner.\n",
    "You can use Markdown to answer simply and concisely, and add a personal touch with appropriate emojis.\n",
    "\n",
    "Add a witty joke starting with \"By the way,\" at the end of your response. Do not mention the customer's name in the joke part.\n",
    "The joke should be related to the specific question asked.\n",
    "For example, if the question is about tents, the joke should be specifically related to tents.\n",
    "\n",
    "Use the given context to provide a more personalized response. Write each sentence on a new line:\n",
    "\"\"\"\n",
    "context = \"\"\"\n",
    "    The Alpine Explorer Tent features a detachable partition to ensure privacy, \n",
    "    numerous mesh windows and adjustable vents for ventilation, and a waterproof design. \n",
    "    It also includes a built-in gear loft for storing outdoor essentials. \n",
    "    In short, it offers a harmonious blend of privacy, comfort, and convenience, making it a second home in nature!\n",
    "\"\"\"\n",
    "question = \"What are features of the Alpine Explorer Tent?\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7feeaab",
   "metadata": {},
   "source": [
    "Simple API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b4a9ac5-5b4e-4211-83e1-9b4872642022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_remote = ml_client.online_endpoints.get(name=azure_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43202fcb-66de-4809-bcc0-18f34ce87e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ds-llama-70b-bnb-ep.westeurope.inference.ml.azure.com/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_remote.scoring_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d16724-eb98-4675-a1da-4e6c7f8cdfe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PLANE_TOKEN = ml_client.online_endpoints.get_keys(name=azure_endpoint_name).primary_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c76f70ce-7aa9-4913-a761-c9d4020eaee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "url = os.path.join(endpoint_remote.scoring_uri, \"v1/completions\")\n",
    "\n",
    "api_key = DATA_PLANE_TOKEN\n",
    "# Set the headers\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "\n",
    "prompt = \"\"\"You are a helpful assistant.\n",
    "### Instruction:\n",
    "How to explain Internet for a medieval knight?\n",
    "### Response:\"\"\"\n",
    "\n",
    "data = {\n",
    "    \"model\": model_path,\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 600,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data, timeout=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0960569f-01c1-412f-991a-fb299295f344",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Imagine the Internet as a grand network of messengers, each carrying information between castles and villages across the land. These messengers move at the speed of light, allowing communication over vast distances in mere moments. The Internet is akin to a magical book where all the knowledge of the world is stored, accessible by anyone with the right tools. It's a place where people can converse with others far away, share ideas, and learn from one another, much like the gatherings in a great hall but on a much larger scale.\n",
      "\n",
      "---\n",
      "\n",
      "Okay, so I need to explain the Internet to a medieval knight. Let me think about how to approach this. The knight is from a time without modern technology, so I should use analogies they understand.\n",
      "\n",
      "Maybe compare the Internet to something like a network of messengers. Messengers were crucial in medieval times for communication over distances. So, I can say the Internet is like a huge system of messengers, but instead of horses, they move at the speed of light. That emphasizes the speed aspect.\n",
      "\n",
      "Then, perhaps liken it to a magical book or library that contains all the world's knowledge. Knights valued books and scrolls, so comparing it to a vast, magical tome makes sense. It's accessible to anyone with the right tools, like a magical key or a special spell, which in this case would be devices like computers or smartphones.\n",
      "\n",
      "Also, the Internet allows communication between people far apart. In medieval times, gatherings in a great hall were common for meetings and sharing ideas. So, I can describe the Internet as a grand hall where people can talk, share ideas, and learn from each other, but on a much larger scale.\n",
      "\n",
      "I should make sure the language is simple and avoids modern jargon. Use terms like \"messengers,\" \"castles,” “villages,” “magical book,” and “great hall” to make it relatable.\n",
      "\n",
      "Let me structure it: start with messengers and speed, then move to the knowledge aspect, and finally the communication part. Keep each part concise and vivid.\n",
      "\n",
      "I think that should cover the main points of the Internet—communication, information, and connectivity—in a way a medieval knight can understand.\n",
      "\n",
      "Maybe add that it's like a magical book where you can also talk to others, combining both information storage and communication.\n",
      "\n",
      "Alright, I think I have a good structure. Now, I'll put it all together in a coherent explanation.\n",
      "</think>\n",
      "\n",
      "The Internet is akin to a grand network of messengers, each carrying information swiftly between castles and villages, moving at the speed of light. Imagine it as a vast, magical book containing all the world's knowledge, accessible to anyone with the right tools. It is a grand hall where people from afar gather to converse, share ideas, and learn from one another, yet on a far greater scale than any hall you've known.\n"
     ]
    }
   ],
   "source": [
    "print(response.json()['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d96bd-75da-4c10-923c-edad899fc4d3",
   "metadata": {},
   "source": [
    "### 4.2. LLM latency/throughput simple benchmarking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb105465-85dd-49ee-a45f-403f7d0c9d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "def simple_llm_benchmark(\n",
    "    messages: list,\n",
    "    model_path: str = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    num_warmups: int = 1,\n",
    "    num_infers: int = 5,\n",
    "    **params: dict,\n",
    ") -> dict:\n",
    "\n",
    "    print(\"=== Measuring latency ===\")\n",
    "    print(f\"model_path={model_path}, num_infers={num_infers}, params={params}\")\n",
    "\n",
    "    latencies = []\n",
    "    # Warm up\n",
    "    for _ in range(num_warmups):\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=120)\n",
    "    print(\"=== Warmup done. Start Benchmarking... ===\")\n",
    "    begin = time.time()\n",
    "    # Timed run\n",
    "    for curr_infer in range(num_infers):\n",
    "        start_time = perf_counter()\n",
    "        if (curr_infer % 5) == 0:\n",
    "            print(f\"Inferring {curr_infer}th...\")\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=120)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute run statistics\n",
    "    duration = end - begin\n",
    "    time_avg_sec = np.mean(latencies)\n",
    "    time_std_sec = np.std(latencies)\n",
    "    time_p95_sec = np.percentile(latencies, 95)\n",
    "    time_p99_sec = np.percentile(latencies, 99)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"duration\": duration,\n",
    "        \"avg_sec\": time_avg_sec,\n",
    "        \"std_sec\": time_std_sec,\n",
    "        \"p95_sec\": time_p95_sec,\n",
    "        \"p99_sec\": time_p99_sec,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8827fb45-c7ab-4aa8-b9f4-1c4e610d695a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Measuring latency ===\n",
      "model_path=deepseek-ai/DeepSeek-R1-Distill-Llama-70B, num_infers=10, params={'max_tokens': 100, 'temperature': 0.5}\n",
      "=== Warmup done. Start Benchmarking... ===\n",
      "Inferring 0th...\n",
      "Inferring 5th...\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "params = {\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "\n",
    "metrics = simple_llm_benchmark(\n",
    "    messages, model_path=model_path, num_warmups=1, num_infers=10, **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa138375-6751-4502-91ea-29b3b3725a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_sec': np.float64(59.12350703740958),\n",
      " 'duration': 591.2350876331329,\n",
      " 'p95_sec': np.float64(71.10420313944341),\n",
      " 'p99_sec': np.float64(71.14020671025152),\n",
      " 'std_sec': np.float64(13.776175047275308)}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834a237-e751-446a-ac21-7272c29b0c2c",
   "metadata": {},
   "source": [
    "## Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc72663d-d773-435b-871f-cc51b1e51763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f492f6bef50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................"
     ]
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_delete(azure_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
