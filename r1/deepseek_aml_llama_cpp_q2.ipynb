{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796cf06d-e4d3-422e-8301-a961e9520f52",
   "metadata": {},
   "source": [
    "# DeepSeek R1 UD-IQ2_XXS llama.cpp serving using the Azure ML Python SDK\n",
    "\n",
    "> [1] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.<br>[2] Please make sure you prepare [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284ebe3",
   "metadata": {},
   "source": [
    "## Download Preprocessed Quantized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97fa6f",
   "metadata": {},
   "source": [
    "You may first install `huggingface-cli` to download huggingface models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117f95e2-f47e-49d1-bb9a-112fee8de978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <Your Token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8c964-a7f8-4b9d-a10d-e145d49096c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download unsloth/DeepSeek-R1-GGUF --quiet --include \"*UD-IQ2_XXS*\" --local-dir DeepSeek-R1-GGUF --cache-dir .cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb139b1d-500c-4ef2-9af3-728f2a5ea05f",
   "metadata": {},
   "source": [
    "## 1. Load config file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5234c47-b3e5-4218-8a98-3988c8991643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 13:51:23,805 - logger - DEBUG - ===== 0. Azure ML Deployment Info =====\n",
      "2025-03-05 13:51:23,805 - logger - DEBUG - AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
      "2025-03-05 13:51:23,805 - logger - DEBUG - AZURE_RESOURCE_GROUP=<your-resource-group>\n",
      "2025-03-05 13:51:23,805 - logger - DEBUG - AZURE_WORKSPACE=<your-workspace-name>\n",
      "2025-03-05 13:51:23,812 - logger - DEBUG - HF_MODEL_NAME_OR_PATH=DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ2_XXS\n",
      "2025-03-05 13:51:23,812 - logger - DEBUG - IS_DEBUG=True\n",
      "2025-03-05 13:51:23,812 - logger - DEBUG - azure_env_name=deepseek-llm-cpp-r1-q2\n",
      "2025-03-05 13:51:23,812 - logger - DEBUG - azure_model_name=deepseek-r1-q2\n",
      "2025-03-05 13:51:23,812 - logger - DEBUG - azure_endpoint_name=deepseek-r1-q2-endpoint\n",
      "2025-03-05 13:51:23,812 - logger - DEBUG - azure_deployment_name=blue\n",
      "2025-03-05 13:51:23,812 - logger - DEBUG - azure_serving_cluster_size=Standard_ND40rs_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from logger import logger\n",
    "from datetime import datetime\n",
    "\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(\"config.yml\") as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "AZURE_SUBSCRIPTION_ID = d[\"config\"][\"AZURE_SUBSCRIPTION_ID\"]\n",
    "AZURE_RESOURCE_GROUP = d[\"config\"][\"AZURE_RESOURCE_GROUP\"]\n",
    "AZURE_WORKSPACE = d[\"config\"][\"AZURE_WORKSPACE\"]\n",
    "HF_TOKEN = ''\n",
    "HF_MODEL_NAME_OR_PATH = d[\"config\"][\"HF_MODEL_NAME_OR_PATH\"]\n",
    "IS_DEBUG = d[\"config\"][\"IS_DEBUG\"]\n",
    "\n",
    "azure_env_name = 'deepseek-llm-cpp-r1-q2'\n",
    "azure_model_name = 'deepseek-r1-q2'\n",
    "azure_endpoint_name = 'deepseek-r1-q2-endpoint'\n",
    "azure_deployment_name = 'blue'\n",
    "azure_serving_cluster_size = 'Standard_ND40rs_v2'\n",
    "\n",
    "\n",
    "if IS_DEBUG:\n",
    "    logger.debug(\"===== 0. Azure ML Deployment Info =====\")\n",
    "    logger.debug(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "    logger.debug(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "    logger.debug(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "    logger.debug(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "    logger.debug(f\"IS_DEBUG={IS_DEBUG}\")\n",
    "\n",
    "    logger.debug(f\"azure_env_name={azure_env_name}\")\n",
    "    logger.debug(f\"azure_model_name={azure_model_name}\")\n",
    "    logger.debug(f\"azure_endpoint_name={azure_endpoint_name}\")\n",
    "    logger.debug(f\"azure_deployment_name={azure_deployment_name}\")\n",
    "    logger.debug(f\"azure_serving_cluster_size={azure_serving_cluster_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9843e0f-3cf1-4e86-abb7-a49919fac8d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Serving preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091d321-a604-48b0-9d0b-e8c4e6855f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-mlflow 1.57.0.post1 requires azure-storage-blob<=12.19.0,>=12.5.0, but you have azure-storage-blob 12.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -q azure-ai-ml azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb4a273-ba31-4f47-a2fd-dc8cdea390f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 01:05:20,030 - logger - INFO - ===== 2. Serving preparation =====\n",
      "2025-03-04 01:05:20,031 - logger - INFO - Calling DefaultAzureCredential.\n",
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "logger.info(f\"===== 2. Serving preparation =====\")\n",
    "logger.info(f\"Calling DefaultAzureCredential.\")\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    ml_client = MLClient(\n",
    "        credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a377b-d10c-413e-a67b-2c11a3cff7fd",
   "metadata": {},
   "source": [
    "### 2.2. Create model asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73532c39-3fdd-40a7-b2be-9f5a2f22443a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    model_name,\n",
    "    job_name=None,\n",
    "    model_dir=\"outputs\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    "):\n",
    "    try:\n",
    "        latest_model_version = max(\n",
    "            [int(m.version) for m in ml_client.models.list(name=model_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Model asset, but will update the Model.\")\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(\n",
    "                name=model_name, version=latest_model_version\n",
    "            )\n",
    "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError, ValueError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        run_model = Model(\n",
    "            name=model_name,\n",
    "            path=model_dir,\n",
    "            description=f\"Deepseek model {HF_MODEL_NAME_OR_PATH}\",\n",
    "            type=model_type,  # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        logger.info(f\"Created Model asset: {model_name} from {model_dir}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aaa9671-fc98-4e5e-a70c-7771caa1c7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 01:08:18,627 - logger - INFO - Exception: (UserError) The specified resource was not found.\n",
      "Code: UserError\n",
      "Message: The specified resource was not found.\n",
      "Exception Details:\t(ModelNotFound) Model container with name: deepseek-r1-q2 not found.\n",
      "\tCode: ModelNotFound\n",
      "\tMessage: Model container with name: deepseek-r1-q2 not found.\n",
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/jihualiu3/code/llm-inferencing/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ2_XXS' 'https://testvllm3226465244.blob.core.windows.net/azureml-blobstore-10331bed-c16d-4680-834e-e054c4eeaeb9/LocalUpload/58609cbc8097dc0357f15b6f8a28407d/DeepSeek-R1-UD-IQ2_XXS' \n",
      "\n",
      "See https://learn.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
      "\u001b[32mUploading DeepSeek-R1-UD-IQ2_XXS (196162.48 MBs): 100%|██████████| 196162482747/196162482747 [15:46<00:00, 207279004.54it/s]\n",
      "\u001b[39m\n",
      "\n",
      "2025-03-04 02:20:45,430 - logger - INFO - Created Model asset: deepseek-r1-q2 from DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ2_XXS\n"
     ]
    }
   ],
   "source": [
    "model = get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    azure_model_name,\n",
    "    job_name=None,\n",
    "    model_dir=\"DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ2_XXS\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df561a-7846-4450-a2dd-4af6396b1719",
   "metadata": {},
   "source": [
    "### 2.3. Create AzureML environment\n",
    "\n",
    "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda). This hands-on uses Docker container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac357a-3944-4702-a338-b9f4b67dadc9",
   "metadata": {},
   "source": [
    "#### Docker environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b19bc6-dc43-4948-8c71-97a033a5f5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 03:16:39,944 - logger - INFO - Exception: (UserError) System.Net.Http.HttpConnectionResponseContent\n",
      "Code: UserError\n",
      "Message: System.Net.Http.HttpConnectionResponseContent\n",
      "2025-03-04 03:16:42,664 - logger - INFO - Created Environment asset: deepseek-llm-cpp-r1-q2\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "\n",
    "\n",
    "def get_or_create_docker_environment_asset(\n",
    "    ml_client, env_name, docker_dir, inference_config=None, update=False\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_env_version = max(\n",
    "            [int(e.version) for e in ml_client.environments.list(name=env_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\n",
    "                \"Found Environment asset, but will update the Environment.\"\n",
    "            )\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(\n",
    "                name=env_name, version=latest_env_version\n",
    "            )\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            build=BuildContext(path=docker_dir),\n",
    "            name=env_name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "            inference_config=inference_config,\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        logger.info(f\"Created Environment asset: {env_name}\")\n",
    "\n",
    "    return env_asset\n",
    "\n",
    "\n",
    "inference_config = {\n",
    "    \"liveness_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/health\",\n",
    "    },\n",
    "    \"readiness_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/health\",\n",
    "    },\n",
    "    \"scoring_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/\",\n",
    "    },\n",
    "}\n",
    "\n",
    "env = get_or_create_docker_environment_asset(\n",
    "    ml_client, azure_env_name, \"docker-r1\", inference_config, update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb187e-370f-4481-9d82-a38ae982c1e3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serving\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Create endpoint\n",
    "\n",
    "Create an endpoint. This process does not provision a GPU cluster yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c22433-4ab8-4db9-956b-7f437b86dfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 03:16:54,432 - logger - INFO - ===== 3. Serving =====\n",
      "2025-03-04 03:17:57,612 - logger - INFO - \n",
      "---Endpoint created successfully---\n",
      "\n",
      "2025-03-04 03:17:57,665 - logger - INFO - Creating Endpoint took 1 minute and 3.21 seconds\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    ")\n",
    "\n",
    "logger.info(f\"===== 3. Serving =====\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Check if the endpoint already exists in the workspace\n",
    "try:\n",
    "    endpoint = ml_client.online_endpoints.get(azure_endpoint_name)\n",
    "    logger.info(\"---Endpoint already exists---\")\n",
    "except:\n",
    "    # Create an online endpoint if it doesn't exist\n",
    "\n",
    "    # Define the endpoint\n",
    "    endpoint = ManagedOnlineEndpoint(\n",
    "        name=azure_endpoint_name,\n",
    "        description=f\"Test endpoint for {model.name}\",\n",
    "    )\n",
    "\n",
    "    # Trigger the endpoint creation\n",
    "    try:\n",
    "        ml_client.begin_create_or_update(endpoint).wait()\n",
    "        logger.info(\"\\n---Endpoint created successfully---\\n\")\n",
    "    except Exception as err:\n",
    "        raise RuntimeError(f\"Endpoint creation failed. Detailed Response:\\n{err}\") from err\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "from humanfriendly import format_timespan\n",
    "\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating Endpoint took {timespan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a05af-0aca-4d36-9c0f-bfa4dcc6203b",
   "metadata": {},
   "source": [
    "### 3.2. Create Deployment\n",
    "\n",
    "Create a Deployment. This takes a lot of time as GPU clusters must be provisioned and the serving environment must be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d08ab32-5cd6-439f-bb34-13ddc200a279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"MODEL_NAME\": '/models/DeepSeek-R1-UD-IQ2_XXS/DeepSeek-R1-UD-IQ2_XXS-00001-of-00004.gguf', ## /var/azureml-app/azureml-models/deepseek-adapter/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "    \"LAYER_N\": \"61\",\n",
    "    \"PREDICT_N\": \"10000\",\n",
    "    \"VLLM_ARGS\": \"\",\n",
    "}\n",
    "deployment_env_vars = {**env_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afaa8b-5af1-49d1-990f-414da0effe8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint deepseek-r1-q2-endpoint exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 03:32:10,707 - logger - INFO - \n",
      "---Deployment created successfully---\n",
      "\n",
      "2025-03-04 03:32:12,449 - logger - INFO - Creating deployment took 14 minutes and 14.76 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 s, sys: 180 ms, total: 2.24 s\n",
      "Wall time: 14min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from azure.ai.ml.entities import (    \n",
    "    OnlineRequestSettings,\n",
    "    CodeConfiguration,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    Environment\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=azure_deployment_name,\n",
    "    endpoint_name=azure_endpoint_name,\n",
    "    model=model,\n",
    "    model_mount_path='/models',\n",
    "    instance_type=azure_serving_cluster_size,\n",
    "    instance_count=1,\n",
    "    environment_variables=deployment_env_vars,    \n",
    "    environment=env,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=2,\n",
    "        request_timeout_ms=120000, \n",
    "        max_queue_wait_ms=240000\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=5,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=30,\n",
    "        initial_delay=120\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        timeout=2,\n",
    "        period=10,\n",
    "        initial_delay=120,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Trigger the deployment creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(deployment).wait()\n",
    "    logger.info(\"\\n---Deployment created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err\n",
    "    \n",
    "endpoint.traffic = {azure_deployment_name: 100}\n",
    "endpoint_poller = ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
    "\n",
    "t1 = time.time()\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating deployment took {timespan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f7dfe9-e27e-449d-a5cb-425663651c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_results = endpoint_poller.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ae58e5f-999f-449e-b5ac-c766c184715f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_mode: key\n",
      "description: Test endpoint for deepseek-r1-q2\n",
      "id: /subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/resourceGroups/dev/providers/Microsoft.MachineLearningServices/workspaces/test-vllm/onlineEndpoints/deepseek-r1-q2-endpoint\n",
      "identity:\n",
      "  principal_id: 2a38e9ed-6182-4ade-97a3-26d5ef1ddb9b\n",
      "  tenant_id: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "  type: system_assigned\n",
      "kind: Managed\n",
      "location: westeurope\n",
      "mirror_traffic: {}\n",
      "name: deepseek-r1-q2-endpoint\n",
      "openapi_uri: https://deepseek-r1-q2-endpoint.westeurope.inference.ml.azure.com/swagger.json\n",
      "properties:\n",
      "  AzureAsyncOperationUri: https://management.azure.com/subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/providers/Microsoft.MachineLearningServices/locations/westeurope/mfeOperationsStatus/oeidp:10331bed-c16d-4680-834e-e054c4eeaeb9:40d67bd9-3824-47cf-a0a6-8f5e5b1ed5f2?api-version=2022-02-01-preview\n",
      "  azureml.onlineendpointid: /subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/resourcegroups/dev/providers/microsoft.machinelearningservices/workspaces/test-vllm/onlineendpoints/deepseek-r1-q2-endpoint\n",
      "  createdAt: 2025-03-04T03:16:56.603852+0000\n",
      "  createdBy: Jihua Liu\n",
      "  lastModifiedAt: 2025-03-04T03:32:12.240742+0000\n",
      "provisioning_state: Succeeded\n",
      "public_network_access: enabled\n",
      "scoring_uri: https://deepseek-r1-q2-endpoint.westeurope.inference.ml.azure.com/\n",
      "tags: {}\n",
      "traffic:\n",
      "  blue: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(endpoint_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de81f846-58e0-4717-92ac-9871787e7a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = endpoint_results.name\n",
    "keys = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "primary_key = keys.primary_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f437f6-153a-42d5-ab22-0011d0fe2481",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Test\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Invocation\n",
    "\n",
    "Try calling the endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f95ab3c1-eee5-42ea-adc1-ced7eaeaf886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "802d8e0f-d579-43f1-8e55-d4506b81c7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "url = os.path.join(endpoint_results.scoring_uri, \"v1\")\n",
    "endpoint_name = (\n",
    "    endpoint_results.name if azure_endpoint_name is None else azure_endpoint_name\n",
    ")\n",
    "keys = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "primary_key = keys.primary_key  # You can paste [YOUR Azure ML API KEY] here\n",
    "llm = OpenAI(base_url=url, api_key=primary_key)\n",
    "model_path = \"/models/DeepSeek-R1-UD-IQ2_XXS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b67088d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create your prompt\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant that helps customers find information. As an assistant, you respond to questions in a concise and unique manner.\n",
    "You can use Markdown to answer simply and concisely, and add a personal touch with appropriate emojis.\n",
    "\n",
    "Add a witty joke starting with \"By the way,\" at the end of your response. Do not mention the customer's name in the joke part.\n",
    "The joke should be related to the specific question asked.\n",
    "For example, if the question is about tents, the joke should be specifically related to tents.\n",
    "\n",
    "Use the given context to provide a more personalized response. Write each sentence on a new line:\n",
    "\"\"\"\n",
    "context = \"\"\"\n",
    "    The Alpine Explorer Tent features a detachable partition to ensure privacy, \n",
    "    numerous mesh windows and adjustable vents for ventilation, and a waterproof design. \n",
    "    It also includes a built-in gear loft for storing outdoor essentials. \n",
    "    In short, it offers a harmonious blend of privacy, comfort, and convenience, making it a second home in nature!\n",
    "\"\"\"\n",
    "question = \"What are features of the Alpine Explorer Tent?\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7feeaab",
   "metadata": {},
   "source": [
    "Simple API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e01c22a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, let's tackle this question about the Alpine Explorer Tent. The user is asking for its features, so I need to go through the context provided.\n",
      "\n",
      "First, the context mentions a detachable partition for privacy. That's a key point. Then there are mesh windows and adjustable vents for ventilation. The waterproof design is another important feature. Also, the built-in gear loft for storage. The summary says it's a blend of privacy, comfort, and convenience, like a second home. \n",
      "\n",
      "I need to list these features clearly. Each sentence on a new line, using markdown. Then add a joke related to tents. The joke should start with \"By the way,\" and not include the user's name. Let me think of a tent joke. Maybe something about camping and not needing a room, since the tent has privacy. \"Why did the tent refuse to gossip? It had too many *vents* to cover!\" Yeah, that works with vents and privacy. Al\n"
     ]
    }
   ],
   "source": [
    "# Simple API Call\n",
    "response = llm.chat.completions.create(\n",
    "    model=model_path,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07562460",
   "metadata": {},
   "source": [
    "Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "229fa4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "<think>\n",
      "Okay, let's see. The user is asking about the features of the Alpine Explorer Tent. Let me go through the context provided again to make sure I don't miss anything.\n",
      "\n",
      "The context mentions a detachable partition for privacy. That's one feature. Then there are numerous mesh windows and adjustable vents, which are for ventilation. So ventilation features include those two points. The tent is waterproof, which is a key design aspect. Also, there's a built-in gear loft for storing outdoor essentials. \n",
      "\n",
      "The summary part says it blends privacy, comfort, and convenience, acting as a second home. I need to list all the specific features from the context. Let me check again to ensure I don't include the summary as a feature itself but extract the actual components. Detachable partition, mesh windows, adjustable vents, waterproof design, built-in gear loft. Those are the explicit features. The rest is more about the benefits or the overall impression. \n",
      "\n",
      "I should present them in aNone"
     ]
    }
   ],
   "source": [
    "response = llm.chat.completions.create(\n",
    "    model=model_path,\n",
    "    messages=[\n",
    "        {\"role\": \"saystem\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    "    stream=True,  # Stream the response\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in response:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if hasattr(delta, \"content\"):\n",
    "        print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff7ee2-d17b-4488-877c-9877f192fca1",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed0e352-5d6f-459d-b27f-3c9f37536ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'text': '7.5-square-mile city where 850,000 people live, work, and play. As the population grows and climate change impacts become more severe, the need for resilient infrastructure and public spaces is critical. San Francisco has been a leader in green infrastructure, with a long history of progressive environmental policies. San Francisco was one of the first cities in the nation to develop a Green Connections plan, which is a citywide network of green, social, and bicycle and pedestrian corridors connecting people to parks, open spaces, and the waterfront. The plan is a result of an extensive community engagement process, and it serves as the city’s green infrastructure plan. San Francisco has also been a leader in stormwater management, with a long history of using green infrastructure to manage stormwater. The city’s Stormwater Management Plan was adopted in 2006, and it includes a comprehensive approach to stormwater management that includes both traditional gray infrastructure and green infrastructure. San Francisco has also been a leader in the', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'created': 1741059276, 'model': '/models/DeepSeek-R1-UD-IQ2_XXS', 'system_fingerprint': 'b4798-1782cdfe', 'object': 'text_completion', 'usage': {'completion_tokens': 200, 'prompt_tokens': 6, 'total_tokens': 206}, 'id': 'chatcmpl-Ep84PINYdk1oxqHFvweYXW138w6yRGuP', 'timings': {'prompt_n': 5, 'prompt_ms': 231.782, 'prompt_per_token_ms': 46.3564, 'prompt_per_second': 21.571994374023866, 'predicted_n': 200, 'predicted_ms': 14718.537, 'predicted_per_token_ms': 73.592685, 'predicted_per_second': 13.588307044375403}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "completions_url = os.path.join(endpoint_results.scoring_uri, \"v1/completions\")\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {primary_key}\"}\n",
    "data = {\n",
    "    \"model\": model_path,\n",
    "    \"prompt\": \"San Francisco is a \",\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "response = requests.post(completions_url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d96bd-75da-4c10-923c-edad899fc4d3",
   "metadata": {},
   "source": [
    "### 4.2. LLM latency/throughput simple benchmarking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb105465-85dd-49ee-a45f-403f7d0c9d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "def simple_llm_benchmark(\n",
    "    llm: OpenAI,\n",
    "    messages: list,\n",
    "    model_path: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    num_warmups: int = 1,\n",
    "    num_infers: int = 5,\n",
    "    **params: dict,\n",
    ") -> dict:\n",
    "\n",
    "    print(\"=== Measuring latency ===\")\n",
    "    print(f\"model_path={model_path}, num_infers={num_infers}, params={params}\")\n",
    "\n",
    "    latencies = []\n",
    "    # Warm up\n",
    "    for _ in range(num_warmups):\n",
    "        response = llm.chat.completions.create(\n",
    "            model=model_path,\n",
    "            messages=messages,\n",
    "            **params,\n",
    "        )\n",
    "    print(\"=== Warmup done. Start Benchmarking... ===\")\n",
    "    begin = time.time()\n",
    "    # Timed run\n",
    "    for curr_infer in range(num_infers):\n",
    "        start_time = perf_counter()\n",
    "        if (curr_infer % 5) == 0:\n",
    "            print(f\"Inferring {curr_infer}th...\")\n",
    "        response = llm.chat.completions.create(\n",
    "            model=model_path,\n",
    "            messages=messages,\n",
    "            **params,\n",
    "        )\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute run statistics\n",
    "    duration = end - begin\n",
    "    time_avg_sec = np.mean(latencies)\n",
    "    time_std_sec = np.std(latencies)\n",
    "    time_p95_sec = np.percentile(latencies, 95)\n",
    "    time_p99_sec = np.percentile(latencies, 99)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"duration\": duration,\n",
    "        \"avg_sec\": time_avg_sec,\n",
    "        \"std_sec\": time_std_sec,\n",
    "        \"p95_sec\": time_p95_sec,\n",
    "        \"p99_sec\": time_p99_sec,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8827fb45-c7ab-4aa8-b9f4-1c4e610d695a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Measuring latency ===\n",
      "model_path=/models/DeepSeek-R1-UD-IQ2_XXS, num_infers=10, params={'max_tokens': 100, 'temperature': 0.5}\n",
      "=== Warmup done. Start Benchmarking... ===\n",
      "Inferring 0th...\n",
      "Inferring 5th...\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "params = {\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "model_path = \"/models/DeepSeek-R1-UD-IQ2_XXS\"\n",
    "\n",
    "metrics = simple_llm_benchmark(\n",
    "    llm, messages, model_path=model_path, num_warmups=1, num_infers=10, **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa138375-6751-4502-91ea-29b3b3725a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_sec': 7.865846022090409,\n",
      " 'duration': 78.65847659111023,\n",
      " 'p95_sec': 8.272176076984033,\n",
      " 'p99_sec': 8.30625159942545,\n",
      " 'std_sec': 0.3825325533353871}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834a237-e751-446a-ac21-7272c29b0c2c",
   "metadata": {},
   "source": [
    "## Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc72663d-d773-435b-871f-cc51b1e51763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7ff529436830>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................"
     ]
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_delete(azure_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
