{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796cf06d-e4d3-422e-8301-a961e9520f52",
   "metadata": {},
   "source": [
    "# DeepSeek R1 Distillation 1.5B vLLM serving using the Azure ML Python SDK\n",
    "\n",
    "> [1] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.<br>[2] Please make sure you prepare [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb139b1d-500c-4ef2-9af3-728f2a5ea05f",
   "metadata": {},
   "source": [
    "## 1. Load config file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f95e2-f47e-49d1-bb9a-112fee8de978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli login --token <your token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65615fa9-0ff5-4720-9d99-8e7034e945da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/jihualiu3/code/llm-inferencing/DeepSeek-R1-GGUF\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download unsloth/DeepSeek-R1-GGUF --quiet --include \"*UD-Q2_K_XL*\" --local-dir DeepSeek-R1-GGUF --cache-dir .cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5234c47-b3e5-4218-8a98-3988c8991643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 13:51:39,450 - logger - DEBUG - ===== 0. Azure ML Deployment Info =====\n",
      "2025-03-05 13:51:39,450 - logger - DEBUG - AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
      "2025-03-05 13:51:39,450 - logger - DEBUG - AZURE_RESOURCE_GROUP=<your-resource-group>\n",
      "2025-03-05 13:51:39,450 - logger - DEBUG - AZURE_WORKSPACE=<your-workspace-name>\n",
      "2025-03-05 13:51:39,450 - logger - DEBUG - HF_MODEL_NAME_OR_PATH=DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL\n",
      "2025-03-05 13:51:39,465 - logger - DEBUG - IS_DEBUG=True\n",
      "2025-03-05 13:51:39,466 - logger - DEBUG - azure_env_name=deepseek-llm-cpp-r1-k\n",
      "2025-03-05 13:51:39,466 - logger - DEBUG - azure_model_name=deepseek-r1-k\n",
      "2025-03-05 13:51:39,466 - logger - DEBUG - azure_endpoint_name=deepseek-r1-k-endpoint\n",
      "2025-03-05 13:51:39,466 - logger - DEBUG - azure_deployment_name=blue\n",
      "2025-03-05 13:51:39,471 - logger - DEBUG - azure_serving_cluster_size=Standard_ND40rs_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from logger import logger\n",
    "from datetime import datetime\n",
    "\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(\"config.yml\") as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "AZURE_SUBSCRIPTION_ID = d[\"config\"][\"AZURE_SUBSCRIPTION_ID\"]\n",
    "AZURE_RESOURCE_GROUP = d[\"config\"][\"AZURE_RESOURCE_GROUP\"]\n",
    "AZURE_WORKSPACE = d[\"config\"][\"AZURE_WORKSPACE\"]\n",
    "HF_TOKEN = ''\n",
    "HF_MODEL_NAME_OR_PATH = d[\"config\"][\"HF_MODEL_NAME_OR_PATH\"]\n",
    "IS_DEBUG = d[\"config\"][\"IS_DEBUG\"]\n",
    "\n",
    "azure_env_name = 'deepseek-llm-cpp-r1-k'\n",
    "azure_model_name = 'deepseek-r1-k'\n",
    "azure_endpoint_name = 'deepseek-r1-k-endpoint'\n",
    "azure_deployment_name = 'blue'\n",
    "azure_serving_cluster_size = 'Standard_ND40rs_v2'\n",
    "\n",
    "\n",
    "if IS_DEBUG:\n",
    "    logger.debug(\"===== 0. Azure ML Deployment Info =====\")\n",
    "    logger.debug(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "    logger.debug(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "    logger.debug(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "    logger.debug(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "    logger.debug(f\"IS_DEBUG={IS_DEBUG}\")\n",
    "\n",
    "    logger.debug(f\"azure_env_name={azure_env_name}\")\n",
    "    logger.debug(f\"azure_model_name={azure_model_name}\")\n",
    "    logger.debug(f\"azure_endpoint_name={azure_endpoint_name}\")\n",
    "    logger.debug(f\"azure_deployment_name={azure_deployment_name}\")\n",
    "    logger.debug(f\"azure_serving_cluster_size={azure_serving_cluster_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9843e0f-3cf1-4e86-abb7-a49919fac8d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Serving preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0091d321-a604-48b0-9d0b-e8c4e6855f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-mlflow 1.57.0.post1 requires azure-storage-blob<=12.19.0,>=12.5.0, but you have azure-storage-blob 12.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q azure-ai-ml azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccb4a273-ba31-4f47-a2fd-dc8cdea390f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 14:47:12,281 - logger - INFO - ===== 2. Serving preparation =====\n",
      "2025-03-02 14:47:12,749 - logger - INFO - Calling DefaultAzureCredential.\n",
      "Found the config file in: /config.json\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "logger.info(f\"===== 2. Serving preparation =====\")\n",
    "logger.info(f\"Calling DefaultAzureCredential.\")\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    ml_client = MLClient(\n",
    "        credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a377b-d10c-413e-a67b-2c11a3cff7fd",
   "metadata": {},
   "source": [
    "### 2.2. Create model asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73532c39-3fdd-40a7-b2be-9f5a2f22443a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    model_name,\n",
    "    job_name=None,\n",
    "    model_dir=\"outputs\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    "):\n",
    "    try:\n",
    "        latest_model_version = max(\n",
    "            [int(m.version) for m in ml_client.models.list(name=model_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Model asset, but will update the Model.\")\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(\n",
    "                name=model_name, version=latest_model_version\n",
    "            )\n",
    "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError, ValueError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        if job_name is None:\n",
    "            model_path = model_dir\n",
    "        else:\n",
    "            model_path = (\n",
    "                f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"\n",
    "            )\n",
    "        run_model = Model(\n",
    "            name=model_name,\n",
    "            path=model_path,\n",
    "            description=\"Model created from run.\",\n",
    "            type=model_type,  # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        logger.info(f\"Created Model asset: {model_name}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa9671-fc98-4e5e-a70c-7771caa1c7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 14:48:46,542 - logger - INFO - Exception: max() arg is an empty sequence\n"
     ]
    }
   ],
   "source": [
    "model = get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    azure_model_name,\n",
    "    job_name=None,\n",
    "    model_dir=\"DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df561a-7846-4450-a2dd-4af6396b1719",
   "metadata": {},
   "source": [
    "### 2.3. Create AzureML environment\n",
    "\n",
    "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda). This hands-on uses Docker container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac357a-3944-4702-a338-b9f4b67dadc9",
   "metadata": {},
   "source": [
    "#### Docker environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b19bc6-dc43-4948-8c71-97a033a5f5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 11:09:34,210 - logger - INFO - Exception: Found Environment asset, but will update the Environment.\n",
      "2025-03-02 11:09:49,595 - logger - INFO - Created Environment asset: deepseek-llm-cpp-r1-k\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "\n",
    "\n",
    "def get_or_create_docker_environment_asset(\n",
    "    ml_client, env_name, docker_dir, inference_config=None, update=False\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_env_version = max(\n",
    "            [int(e.version) for e in ml_client.environments.list(name=env_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\n",
    "                \"Found Environment asset, but will update the Environment.\"\n",
    "            )\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(\n",
    "                name=env_name, version=latest_env_version\n",
    "            )\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            build=BuildContext(path=docker_dir),\n",
    "            name=env_name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "            inference_config=inference_config,\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        logger.info(f\"Created Environment asset: {env_name}\")\n",
    "\n",
    "    return env_asset\n",
    "\n",
    "\n",
    "inference_config = {\n",
    "    \"liveness_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/health\",\n",
    "    },\n",
    "    \"readiness_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/health\",\n",
    "    },\n",
    "    \"scoring_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/\",\n",
    "    },\n",
    "}\n",
    "\n",
    "env = get_or_create_docker_environment_asset(\n",
    "    ml_client, azure_env_name, \"docker-r1\", inference_config, update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb187e-370f-4481-9d82-a38ae982c1e3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serving\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Create endpoint\n",
    "\n",
    "Create an endpoint. This process does not provision a GPU cluster yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c22433-4ab8-4db9-956b-7f437b86dfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 00:44:57,284 - logger - INFO - ===== 3. Serving =====\n",
      "2025-03-03 00:46:29,877 - logger - INFO - \n",
      "---Endpoint created successfully---\n",
      "\n",
      "2025-03-03 00:46:29,878 - logger - INFO - Creating Endpoint took 1 minute and 32.59 seconds\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    ")\n",
    "\n",
    "logger.info(f\"===== 3. Serving =====\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Check if the endpoint already exists in the workspace\n",
    "try:\n",
    "    endpoint = ml_client.online_endpoints.get(azure_endpoint_name)\n",
    "    logger.info(\"---Endpoint already exists---\")\n",
    "except:\n",
    "    # Create an online endpoint if it doesn't exist\n",
    "\n",
    "    # Define the endpoint\n",
    "    endpoint = ManagedOnlineEndpoint(\n",
    "        name=azure_endpoint_name,\n",
    "        description=f\"Test endpoint for {model.name}\",\n",
    "    )\n",
    "\n",
    "# Trigger the endpoint creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(endpoint).wait()\n",
    "    logger.info(\"\\n---Endpoint created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(f\"Endpoint creation failed. Detailed Response:\\n{err}\") from err\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "from humanfriendly import format_timespan\n",
    "\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating Endpoint took {timespan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a05af-0aca-4d36-9c0f-bfa4dcc6203b",
   "metadata": {},
   "source": [
    "### 3.2. Create Deployment\n",
    "\n",
    "Create a Deployment. This takes a lot of time as GPU clusters must be provisioned and the serving environment must be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d08ab32-5cd6-439f-bb34-13ddc200a279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"MODEL_NAME\": '/models/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf', ## /var/azureml-app/azureml-models/deepseek-adapter/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "    \"LAYER_N\": \"50\",\n",
    "    \"PREDICT_N\": \"10000\",\n",
    "    \"VLLM_ARGS\": \"\"\n",
    "}\n",
    "deployment_env_vars = {**env_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afaa8b-5af1-49d1-990f-414da0effe8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint deepseek-r1-k-endpoint exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................................................................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 01:07:16,008 - logger - INFO - \n",
      "---Deployment created successfully---\n",
      "\n",
      "2025-03-03 01:07:17,500 - logger - INFO - Creating deployment took 18 minutes and 53.22 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.78 s, sys: 183 ms, total: 2.97 s\n",
      "Wall time: 18min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from azure.ai.ml.entities import (    \n",
    "    OnlineRequestSettings,\n",
    "    CodeConfiguration,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    Environment\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=azure_deployment_name,\n",
    "    endpoint_name=azure_endpoint_name,\n",
    "    model=model,\n",
    "    model_mount_path='/models',\n",
    "    instance_type=azure_serving_cluster_size,\n",
    "    instance_count=1,\n",
    "    environment_variables=deployment_env_vars,    \n",
    "    environment=env,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=2,\n",
    "        request_timeout_ms=180000, \n",
    "        max_queue_wait_ms=360000\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=5,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=30,\n",
    "        initial_delay=120\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        timeout=2,\n",
    "        period=10,\n",
    "        initial_delay=120,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Trigger the deployment creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(deployment).wait()\n",
    "    logger.info(\"\\n---Deployment created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err\n",
    "    \n",
    "endpoint.traffic = {azure_deployment_name: 100}\n",
    "endpoint_poller = ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
    "\n",
    "t1 = time.time()\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating deployment took {timespan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23f7dfe9-e27e-449d-a5cb-425663651c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_results = endpoint_poller.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ae58e5f-999f-449e-b5ac-c766c184715f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_mode: key\n",
      "description: Test endpoint for deepseek-r1-k\n",
      "id: /subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/resourceGroups/dev/providers/Microsoft.MachineLearningServices/workspaces/test-vllm/onlineEndpoints/deepseek-r1-k-endpoint\n",
      "identity:\n",
      "  principal_id: fd1d162f-8998-49bc-a173-1fd86eac0546\n",
      "  tenant_id: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "  type: system_assigned\n",
      "kind: Managed\n",
      "location: westeurope\n",
      "mirror_traffic: {}\n",
      "name: deepseek-r1-k-endpoint\n",
      "openapi_uri: https://deepseek-r1-k-endpoint.westeurope.inference.ml.azure.com/swagger.json\n",
      "properties:\n",
      "  AzureAsyncOperationUri: https://management.azure.com/subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/providers/Microsoft.MachineLearningServices/locations/westeurope/mfeOperationsStatus/oeidp:10331bed-c16d-4680-834e-e054c4eeaeb9:70d1ddb2-a0e4-47b7-89b4-637b17ad918c?api-version=2022-02-01-preview\n",
      "  azureml.onlineendpointid: /subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/resourcegroups/dev/providers/microsoft.machinelearningservices/workspaces/test-vllm/onlineendpoints/deepseek-r1-k-endpoint\n",
      "  createdAt: 2025-03-03T00:44:58.991959+0000\n",
      "  createdBy: Jihua Liu\n",
      "  lastModifiedAt: 2025-03-03T01:07:17.254434+0000\n",
      "provisioning_state: Succeeded\n",
      "public_network_access: enabled\n",
      "scoring_uri: https://deepseek-r1-k-endpoint.westeurope.inference.ml.azure.com/\n",
      "tags: {}\n",
      "traffic:\n",
      "  blue: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(endpoint_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de81f846-58e0-4717-92ac-9871787e7a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = endpoint_results.name\n",
    "keys = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "primary_key = keys.primary_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f437f6-153a-42d5-ab22-0011d0fe2481",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Test\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Invocation\n",
    "\n",
    "Try calling the endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f95ab3c1-eee5-42ea-adc1-ced7eaeaf886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "802d8e0f-d579-43f1-8e55-d4506b81c7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "url = os.path.join(endpoint_results.scoring_uri, \"v1\")\n",
    "endpoint_name = (\n",
    "    endpoint_results.name if azure_endpoint_name is None else azure_endpoint_name\n",
    ")\n",
    "keys = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "primary_key = keys.primary_key  # You can paste [YOUR Azure ML API KEY] here\n",
    "llm = OpenAI(base_url=url, api_key=primary_key)\n",
    "model_path = \"/models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b67088d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create your prompt\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant that helps customers find information. As an assistant, you respond to questions in a concise and unique manner.\n",
    "You can use Markdown to answer simply and concisely, and add a personal touch with appropriate emojis.\n",
    "\n",
    "Add a witty joke starting with \"By the way,\" at the end of your response. Do not mention the customer's name in the joke part.\n",
    "The joke should be related to the specific question asked.\n",
    "For example, if the question is about tents, the joke should be specifically related to tents.\n",
    "\n",
    "Use the given context to provide a more personalized response. Write each sentence on a new line:\n",
    "\"\"\"\n",
    "context = \"\"\"\n",
    "    The Alpine Explorer Tent features a detachable partition to ensure privacy, \n",
    "    numerous mesh windows and adjustable vents for ventilation, and a waterproof design. \n",
    "    It also includes a built-in gear loft for storing outdoor essentials. \n",
    "    In short, it offers a harmonious blend of privacy, comfort, and convenience, making it a second home in nature!\n",
    "\"\"\"\n",
    "question = \"What are features of the Alpine Explorer Tent?\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7feeaab",
   "metadata": {},
   "source": [
    "Simple API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e01c22a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this question. The user is asking about the features of the Alpine Explorer Tent. First, I need to go through the provided context to pick out the key points. The context mentions a detachable partition for privacy, mesh windows, adjustable vents for ventilation, a waterproof design, and a built-in gear loft. Also, it says it's like a second home in nature.\n",
      "\n",
      "Now, I should present these features clearly and concisely. Each feature on a new line as per the user's instruction. Then, add a joke related to camping or tents. The joke needs to start with \"By the way,\" and not mention the customer's name. Let me think of a tent-related pun. Maybe something about campers and poles, like \"why don’t campers argue? They prefer to stick to their poles!\" That works because tents have poles and it's a play on words. I'll place the joke at the end after the emoji\n"
     ]
    }
   ],
   "source": [
    "# Simple API Call\n",
    "response = llm.chat.completions.create(\n",
    "    model=model_path,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07562460",
   "metadata": {},
   "source": [
    "Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "229fa4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "<think>\n",
      "Okay, let me see. I need to figure out the features of the Alpine Explorer Tent based on the context provided. Let me read through the context again carefully.\n",
      "\n",
      "The context says it has a detachable partition for privacy. That's one feature. Then there are numerous mesh windows and adjustable vents for ventilation. So ventilation is covered by those. Also, it's a waterproof design, which is important for a tent. There's a built-in gear loft for storing outdoor essentials. The conclusion mentions a blend of privacy, comfort, and convenience, making it like a second home. \n",
      "\n",
      "So the main points are: detachable partition, mesh windows, adjustable vents, waterproof design, built-in gear loft. I should list these as features. The user might want each feature clearly listed, maybe in bullet points. Let me make sure I didn't miss anything. The context mentions \"in short\" but the features before that are the key points. The gear loft is specific for storage. Privacy through the partition, ventilation through windows and vents, waterproofing, and storage with the loft. Yeah, that's all covered. No mention of other features like poles or floor type, so just stick to what's given. I should present these features clearly.\n",
      "</think>\n",
      "\n",
      "The Alpine Explorer Tent includes the following features:  \n",
      "\n",
      "1. **Detachable Partition** – Ensures privacy by allowing the tent to be divided into separate areas.  \n",
      "2. **Mesh Windows & Adjustable Vents** – Promote airflow and ventilation while keeping insects out.  \n",
      "3. **Waterproof Design** – Protects against rain and moisture for reliable weather resistance.  \n",
      "4. **Built-in Gear Loft** – Provides storage space for outdoor essentials, optimizing interior organization.  \n",
      "\n",
      "These elements combine to create a comfortable, private, and convenient camping experience, described as a \"second home in nature.\"None"
     ]
    }
   ],
   "source": [
    "response = llm.chat.completions.create(\n",
    "    model=model_path,\n",
    "    messages=[\n",
    "        {\"role\": \"saystem\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=6000,\n",
    "    stream=True,  # Stream the response\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in response:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if hasattr(delta, \"content\"):\n",
    "        print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff7ee2-d17b-4488-877c-9877f192fca1",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bed0e352-5d6f-459d-b27f-3c9f37536ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'text': '7×7-mile city located at the tip of a peninsula between the Pacific Ocean and San Francisco Bay. The city is known for its steep hills, eclectic mix of architecture, and landmarks, including the Golden Gate Bridge, cable cars, Alcatraz Island, and its Chinatown district.\\n\\nThe Golden Gate Bridge is a suspension bridge spanning the Golden Gate, the one-mile-wide strait connecting San Francisco Bay and the Pacific Ocean. The bridge is one of the most internationally recognized symbols of the United States and is one of the most photographed bridges in the world.\\n\\nAlcatraz Island is located in San Francisco Bay, 1.5 miles offshore from San Francisco. The small island was the site of a military prison from 1859 to 1933, and a federal prison from 1934 to 1963. Today, the island is a national recreation area and is open to visitors.\\n\\nChinatown in San Francisco is the oldest and largest Chinatown in North America. The', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'created': 1740966867, 'model': '/models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL', 'system_fingerprint': 'b4798-1782cdfe', 'object': 'text_completion', 'usage': {'completion_tokens': 200, 'prompt_tokens': 6, 'total_tokens': 206}, 'id': 'chatcmpl-FO4jzNhJ8deWrTra00FL8AFTEVFlqXVz', 'timings': {'prompt_n': 5, 'prompt_ms': 370.458, 'prompt_per_token_ms': 74.0916, 'prompt_per_second': 13.496806655545297, 'predicted_n': 200, 'predicted_ms': 37742.403, 'predicted_per_token_ms': 188.71201499999998, 'predicted_per_second': 5.299079658494453}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "completions_url = os.path.join(endpoint_results.scoring_uri, \"v1/completions\")\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {primary_key}\"}\n",
    "data = {\n",
    "    \"model\": model_path,\n",
    "    \"prompt\": \"San Francisco is a \",\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "response = requests.post(completions_url, headers=headers, json=data, timeout=300)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d96bd-75da-4c10-923c-edad899fc4d3",
   "metadata": {},
   "source": [
    "### 4.2. LLM latency/throughput simple benchmarking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb105465-85dd-49ee-a45f-403f7d0c9d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "def simple_llm_benchmark(\n",
    "    llm: OpenAI,\n",
    "    messages: list,\n",
    "    model_path: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    num_warmups: int = 1,\n",
    "    num_infers: int = 5,\n",
    "    **params: dict,\n",
    ") -> dict:\n",
    "\n",
    "    print(\"=== Measuring latency ===\")\n",
    "    print(f\"model_path={model_path}, num_infers={num_infers}, params={params}\")\n",
    "\n",
    "    latencies = []\n",
    "    # Warm up\n",
    "    for _ in range(num_warmups):\n",
    "        response = llm.chat.completions.create(\n",
    "            model=model_path,\n",
    "            messages=messages,\n",
    "            **params,\n",
    "        )\n",
    "    print(\"=== Warmup done. Start Benchmarking... ===\")\n",
    "    begin = time.time()\n",
    "    # Timed run\n",
    "    for curr_infer in range(num_infers):\n",
    "        start_time = perf_counter()\n",
    "        if (curr_infer % 5) == 0:\n",
    "            print(f\"Inferring {curr_infer}th...\")\n",
    "        response = llm.chat.completions.create(\n",
    "            model=model_path,\n",
    "            messages=messages,\n",
    "            **params,\n",
    "        )\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute run statistics\n",
    "    duration = end - begin\n",
    "    time_avg_sec = np.mean(latencies)\n",
    "    time_std_sec = np.std(latencies)\n",
    "    time_p95_sec = np.percentile(latencies, 95)\n",
    "    time_p99_sec = np.percentile(latencies, 99)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"duration\": duration,\n",
    "        \"avg_sec\": time_avg_sec,\n",
    "        \"std_sec\": time_std_sec,\n",
    "        \"p95_sec\": time_p95_sec,\n",
    "        \"p99_sec\": time_p99_sec,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8827fb45-c7ab-4aa8-b9f4-1c4e610d695a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Measuring latency ===\n",
      "model_path=/models/deepseek-adapter/DeepSeek-R1-Distill-Qwen-1.5B, num_infers=10, params={'max_tokens': 100, 'temperature': 0.5}\n",
      "=== Warmup done. Start Benchmarking... ===\n",
      "Inferring 0th...\n",
      "Inferring 5th...\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "params = {\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "model_path = \"/models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL\"\n",
    "\n",
    "metrics = simple_llm_benchmark(\n",
    "    llm, messages, model_path=model_path, num_warmups=1, num_infers=10, **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa138375-6751-4502-91ea-29b3b3725a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_sec': 24.593713035504333,\n",
      " 'duration': 245.93714904785156,\n",
      " 'p95_sec': 27.716650118958203,\n",
      " 'p99_sec': 28.64820212058723,\n",
      " 'std_sec': 2.31673521689894}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834a237-e751-446a-ac21-7272c29b0c2c",
   "metadata": {},
   "source": [
    "## Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc72663d-d773-435b-871f-cc51b1e51763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7fc1629902e0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................."
     ]
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_delete(azure_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832b154-0056-495d-9702-d266f7228298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
