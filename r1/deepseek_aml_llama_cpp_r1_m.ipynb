{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796cf06d-e4d3-422e-8301-a961e9520f52",
   "metadata": {},
   "source": [
    "# DeepSeek R1-UD-IQ1_M llama.cpp serving using the Azure ML Python SDK\n",
    "\n",
    "> [1] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.<br>[2] Please make sure you prepare [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929a49e",
   "metadata": {},
   "source": [
    "## Download Preprocessed Quantized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e66ff0",
   "metadata": {},
   "source": [
    "You may first install `huggingface-cli` to download huggingface models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f95e2-f47e-49d1-bb9a-112fee8de978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <your token>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbcd1c",
   "metadata": {},
   "source": [
    "Now you can download your model as model assets in Azure Storage. Below will download `DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_M` for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8e3b5-5a8a-48c7-b381-f641dff5c411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download unsloth/DeepSeek-R1-GGUF --include \"*UD-IQ1_M*\" --local-dir DeepSeek-R1-GGUF --cache-dir .cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb139b1d-500c-4ef2-9af3-728f2a5ea05f",
   "metadata": {},
   "source": [
    "## 1. Load config file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5234c47-b3e5-4218-8a98-3988c8991643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 13:51:09,079 - logger - DEBUG - ===== 0. Azure ML Deployment Info =====\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - AZURE_RESOURCE_GROUP=<your-resource-group>\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - AZURE_WORKSPACE=<your-workspace-name>\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - HF_MODEL_NAME_OR_PATH=DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_M\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - IS_DEBUG=True\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - azure_env_name=deepseek-llm-cpp\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - azure_model_name=deepseek-r1-m\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - azure_endpoint_name=deepseek-r1-m-endpoint\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - azure_deployment_name=blue\n",
      "2025-03-05 13:51:09,079 - logger - DEBUG - azure_serving_cluster_size=Standard_ND40rs_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from logger import logger\n",
    "from datetime import datetime\n",
    "\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(\"config.yml\") as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "AZURE_SUBSCRIPTION_ID = d[\"config\"][\"AZURE_SUBSCRIPTION_ID\"]\n",
    "AZURE_RESOURCE_GROUP = d[\"config\"][\"AZURE_RESOURCE_GROUP\"]\n",
    "AZURE_WORKSPACE = d[\"config\"][\"AZURE_WORKSPACE\"]\n",
    "HF_TOKEN = ''\n",
    "HF_MODEL_NAME_OR_PATH = d[\"config\"][\"HF_MODEL_NAME_OR_PATH\"]\n",
    "IS_DEBUG = d[\"config\"][\"IS_DEBUG\"]\n",
    "\n",
    "azure_env_name = 'deepseek-llm-cpp'\n",
    "azure_model_name = 'deepseek-r1-m'\n",
    "azure_endpoint_name = 'deepseek-r1-m-endpoint'\n",
    "azure_deployment_name = 'blue'\n",
    "azure_serving_cluster_size = 'Standard_ND40rs_v2'\n",
    "\n",
    "if IS_DEBUG:\n",
    "    logger.debug(\"===== 0. Azure ML Deployment Info =====\")\n",
    "    logger.debug(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "    logger.debug(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "    logger.debug(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "    logger.debug(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "    logger.debug(f\"IS_DEBUG={IS_DEBUG}\")\n",
    "\n",
    "    logger.debug(f\"azure_env_name={azure_env_name}\")\n",
    "    logger.debug(f\"azure_model_name={azure_model_name}\")\n",
    "    logger.debug(f\"azure_endpoint_name={azure_endpoint_name}\")\n",
    "    logger.debug(f\"azure_deployment_name={azure_deployment_name}\")\n",
    "    logger.debug(f\"azure_serving_cluster_size={azure_serving_cluster_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9843e0f-3cf1-4e86-abb7-a49919fac8d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Serving preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0091d321-a604-48b0-9d0b-e8c4e6855f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q azure-ai-ml azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb4a273-ba31-4f47-a2fd-dc8cdea390f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:38:01,054 - logger - INFO - ===== 2. Serving preparation =====\n",
      "2025-03-02 07:38:01,055 - logger - INFO - Calling DefaultAzureCredential.\n",
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "logger.info(f\"===== 2. Serving preparation =====\")\n",
    "logger.info(f\"Calling DefaultAzureCredential.\")\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    ml_client = MLClient(\n",
    "        credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a377b-d10c-413e-a67b-2c11a3cff7fd",
   "metadata": {},
   "source": [
    "### 2.2. Create model asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73532c39-3fdd-40a7-b2be-9f5a2f22443a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    model_name,\n",
    "    job_name=None,\n",
    "    model_dir=\"outputs\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    "):\n",
    "    try:\n",
    "        latest_model_version = max(\n",
    "            [int(m.version) for m in ml_client.models.list(name=model_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Model asset, but will update the Model.\")\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(\n",
    "                name=model_name, version=latest_model_version\n",
    "            )\n",
    "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        if job_name is None:\n",
    "            model_path = model_dir\n",
    "        else:\n",
    "            model_path = (\n",
    "                f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"\n",
    "            )\n",
    "        run_model = Model(\n",
    "            name=model_name,\n",
    "            path=model_path,\n",
    "            description=\"Model created from run.\",\n",
    "            type=model_type,  # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        logger.info(f\"Created Model asset: {model_name}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa9671-fc98-4e5e-a70c-7771caa1c7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:38:26,957 - logger - INFO - Exception: (UserError) The specified resource was not found.\n",
      "Code: UserError\n",
      "Message: The specified resource was not found.\n",
      "Exception Details:\t(ModelNotFound) Model container with name: deepseek-r1-m not found.\n",
      "\tCode: ModelNotFound\n",
      "\tMessage: Model container with name: deepseek-r1-m not found.\n",
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/jihualiu3/code/llm-inferencing/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_M' 'https://testvllm3226465244.blob.core.windows.net/azureml-blobstore-10331bed-c16d-4680-834e-e054c4eeaeb9/LocalUpload/3f4231739782c924996c973854896c6c/DeepSeek-R1-UD-IQ1_M' \n",
      "\n",
      "See https://learn.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
      "\u001b[32mUploading DeepSeek-R1-UD-IQ1_M (168916.28 MBs):  12%|█▏        | 19942643040/168916283648 [06:10<44:03, 56362309.45it/s]"
     ]
    }
   ],
   "source": [
    "model = get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    azure_model_name,\n",
    "    job_name=None,\n",
    "    model_dir=\"DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_M\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df561a-7846-4450-a2dd-4af6396b1719",
   "metadata": {},
   "source": [
    "### 2.3. Create AzureML environment\n",
    "\n",
    "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda). This hands-on uses Docker container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac357a-3944-4702-a338-b9f4b67dadc9",
   "metadata": {},
   "source": [
    "#### Docker environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7b19bc6-dc43-4948-8c71-97a033a5f5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 09:50:35,847 - logger - INFO - Exception: Found Environment asset, but will update the Environment.\n",
      "\u001b[32mUploading docker-r1 (0.0 MBs): 100%|██████████| 1072/1072 [00:00<00:00, 12249.71it/s]\n",
      "\u001b[39m\n",
      "\n",
      "2025-03-02 09:50:45,869 - logger - INFO - Created Environment asset: deepseek-llm-cpp\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "\n",
    "\n",
    "def get_or_create_docker_environment_asset(\n",
    "    ml_client, env_name, docker_dir, inference_config=None, update=False\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_env_version = max(\n",
    "            [int(e.version) for e in ml_client.environments.list(name=env_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\n",
    "                \"Found Environment asset, but will update the Environment.\"\n",
    "            )\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(\n",
    "                name=env_name, version=latest_env_version\n",
    "            )\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            build=BuildContext(path=docker_dir),\n",
    "            name=env_name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "            inference_config=inference_config,\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        logger.info(f\"Created Environment asset: {env_name}\")\n",
    "\n",
    "    return env_asset\n",
    "\n",
    "\n",
    "inference_config = {\n",
    "    \"liveness_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/health\",\n",
    "    },\n",
    "    \"readiness_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/health\",\n",
    "    },\n",
    "    \"scoring_route\": {\n",
    "        \"port\": 8000,\n",
    "        \"path\": \"/\",\n",
    "    },\n",
    "}\n",
    "\n",
    "env = get_or_create_docker_environment_asset(\n",
    "    ml_client, azure_env_name, \"docker-r1\", inference_config, update=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb187e-370f-4481-9d82-a38ae982c1e3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serving\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Create endpoint\n",
    "\n",
    "Create an endpoint. This process does not provision a GPU cluster yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c22433-4ab8-4db9-956b-7f437b86dfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 09:50:45,911 - logger - INFO - ===== 3. Serving =====\n",
      "2025-03-02 09:50:46,286 - logger - INFO - ---Endpoint already exists---\n",
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "2025-03-02 09:52:18,738 - logger - INFO - \n",
      "---Endpoint created successfully---\n",
      "\n",
      "2025-03-02 09:52:18,751 - logger - INFO - Creating Endpoint took 1 minute and 32.81 seconds\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    ")\n",
    "\n",
    "logger.info(f\"===== 3. Serving =====\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Check if the endpoint already exists in the workspace\n",
    "try:\n",
    "    endpoint = ml_client.online_endpoints.get(azure_endpoint_name)\n",
    "    logger.info(\"---Endpoint already exists---\")\n",
    "except:\n",
    "    # Create an online endpoint if it doesn't exist\n",
    "\n",
    "    # Define the endpoint\n",
    "    endpoint = ManagedOnlineEndpoint(\n",
    "        name=azure_endpoint_name,\n",
    "        description=f\"Test endpoint for {model.name}\",\n",
    "    )\n",
    "\n",
    "    # Trigger the endpoint creation\n",
    "    try:\n",
    "        ml_client.begin_create_or_update(endpoint).wait()\n",
    "        logger.info(\"\\n---Endpoint created successfully---\\n\")\n",
    "    except Exception as err:\n",
    "        raise RuntimeError(f\"Endpoint creation failed. Detailed Response:\\n{err}\") from err\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "from humanfriendly import format_timespan\n",
    "\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating Endpoint took {timespan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a05af-0aca-4d36-9c0f-bfa4dcc6203b",
   "metadata": {},
   "source": [
    "### 3.2. Create Deployment\n",
    "\n",
    "Create a Deployment. This takes a lot of time as GPU clusters must be provisioned and the serving environment must be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d08ab32-5cd6-439f-bb34-13ddc200a279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"MODEL_NAME\": '/models/DeepSeek-R1-UD-IQ1_M/DeepSeek-R1-UD-IQ1_M-00001-of-00004.gguf', ## /var/azureml-app/azureml-models/deepseek-adapter/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "    \"LAYER_N\": \"61\",\n",
    "    \"PREDICT_N\": \"10000\",\n",
    "    \"VLLM_ARGS\": \"\",\n",
    "}\n",
    "deployment_env_vars = {**env_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afaa8b-5af1-49d1-990f-414da0effe8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint deepseek-r1-m-endpoint exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 10:08:15,533 - logger - INFO - \n",
      "---Deployment created successfully---\n",
      "\n",
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "2025-03-02 10:08:17,075 - logger - INFO - Creating deployment took 15 minutes and 58.26 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.3 s, sys: 231 ms, total: 2.54 s\n",
      "Wall time: 15min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from azure.ai.ml.entities import (    \n",
    "    OnlineRequestSettings,\n",
    "    CodeConfiguration,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    Environment\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=azure_deployment_name,\n",
    "    endpoint_name=azure_endpoint_name,\n",
    "    model=model,\n",
    "    model_mount_path='/models',\n",
    "    instance_type=azure_serving_cluster_size,\n",
    "    instance_count=1,\n",
    "    environment_variables=deployment_env_vars,    \n",
    "    environment=env,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=2,\n",
    "        request_timeout_ms=120000, \n",
    "        max_queue_wait_ms=240000\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=5,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=30,\n",
    "        initial_delay=120\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        timeout=2,\n",
    "        period=10,\n",
    "        initial_delay=120,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Trigger the deployment creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(deployment).wait()\n",
    "    logger.info(\"\\n---Deployment created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err\n",
    "    \n",
    "endpoint.traffic = {azure_deployment_name: 100}\n",
    "endpoint_poller = ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
    "\n",
    "t1 = time.time()\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating deployment took {timespan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23f7dfe9-e27e-449d-a5cb-425663651c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_results = endpoint_poller.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ae58e5f-999f-449e-b5ac-c766c184715f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_mode: key\n",
      "description: Test endpoint for deepseek-r1-m\n",
      "id: /subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/resourceGroups/dev/providers/Microsoft.MachineLearningServices/workspaces/test-vllm/onlineEndpoints/deepseek-r1-m-endpoint\n",
      "identity:\n",
      "  principal_id: 5128e5cf-e952-496e-beeb-ebff815a1c1c\n",
      "  tenant_id: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "  type: system_assigned\n",
      "kind: Managed\n",
      "location: westeurope\n",
      "mirror_traffic: {}\n",
      "name: deepseek-r1-m-endpoint\n",
      "openapi_uri: https://deepseek-r1-m-endpoint.westeurope.inference.ml.azure.com/swagger.json\n",
      "properties:\n",
      "  AzureAsyncOperationUri: https://management.azure.com/subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/providers/Microsoft.MachineLearningServices/locations/westeurope/mfeOperationsStatus/oeidp:10331bed-c16d-4680-834e-e054c4eeaeb9:4ed34a2c-e898-4e22-a232-a94b2a3b976e?api-version=2022-02-01-preview\n",
      "  azureml.onlineendpointid: /subscriptions/e56790f8-0506-49eb-95b8-82817828d59d/resourcegroups/dev/providers/microsoft.machinelearningservices/workspaces/test-vllm/onlineendpoints/deepseek-r1-m-endpoint\n",
      "  createdAt: 2025-03-02T09:06:01.353202+0000\n",
      "  createdBy: Jihua Liu\n",
      "  lastModifiedAt: 2025-03-02T10:08:16.802847+0000\n",
      "provisioning_state: Succeeded\n",
      "public_network_access: enabled\n",
      "scoring_uri: https://deepseek-r1-m-endpoint.westeurope.inference.ml.azure.com/\n",
      "tags: {}\n",
      "traffic:\n",
      "  blue: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(endpoint_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de81f846-58e0-4717-92ac-9871787e7a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = endpoint_results.name\n",
    "keys = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "primary_key = keys.primary_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f437f6-153a-42d5-ab22-0011d0fe2481",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Test\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Invocation\n",
    "\n",
    "Try calling the endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f95ab3c1-eee5-42ea-adc1-ced7eaeaf886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "802d8e0f-d579-43f1-8e55-d4506b81c7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "url = os.path.join(endpoint_results.scoring_uri, \"v1\")\n",
    "endpoint_name = (\n",
    "    endpoint_results.name if azure_endpoint_name is None else azure_endpoint_name\n",
    ")\n",
    "keys = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "primary_key = keys.primary_key  # You can paste [YOUR Azure ML API KEY] here\n",
    "llm = OpenAI(base_url=url, api_key=primary_key)\n",
    "model_path = \"/models/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ2_XXS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b67088d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create your prompt\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant that helps customers find information. As an assistant, you respond to questions in a concise and unique manner.\n",
    "You can use Markdown to answer simply and concisely, and add a personal touch with appropriate emojis.\n",
    "\n",
    "Add a witty joke starting with \"By the way,\" at the end of your response. Do not mention the customer's name in the joke part.\n",
    "The joke should be related to the specific question asked.\n",
    "For example, if the question is about tents, the joke should be specifically related to tents.\n",
    "\n",
    "Use the given context to provide a more personalized response. Write each sentence on a new line:\n",
    "\"\"\"\n",
    "context = \"\"\"\n",
    "    The Alpine Explorer Tent features a detachable partition to ensure privacy, \n",
    "    numerous mesh windows and adjustable vents for ventilation, and a waterproof design. \n",
    "    It also includes a built-in gear loft for storing outdoor essentials. \n",
    "    In short, it offers a harmonious blend of privacy, comfort, and convenience, making it a second home in nature!\n",
    "\"\"\"\n",
    "question = \"What are features of the Alpine Explorer Tent?\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7feeaab",
   "metadata": {},
   "source": [
    "Simple API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e01c22a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about the features of the Alpine Explorer Tent. Let me check the context provided. The tent has a detachable partition for privacy, several mesh windows and adjustable vents for ventilation, a waterproof design, and a built-in gear loft. I need to list these features clearly.\n",
      "\n",
      "I should start each sentence on a new line and use emojis to make it friendly. Then add a joke related to tents starting with \"By the way,\" without mentioning the user's name. The joke should be witty and connected to the features. Maybe something about the tent being so nice it doesn’t want to leave? Or maybe about the gear loft being a secret storage spot. Hmm, the gear loft is for essentials, so a joke about hiding snacks there. That’s light and relatable. Perfect.\n",
      "</think>\n",
      "\n",
      "The Alpine Explorer Tent boasts a detachable partition for privacy 🏕️  \n",
      "Multiple mesh windows and adjustable vents keep the airflow just right 🌬\n"
     ]
    }
   ],
   "source": [
    "# Simple API Call\n",
    "response = llm.chat.completions.create(\n",
    "    model=model_path,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07562460",
   "metadata": {},
   "source": [
    "Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "229fa4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "<think>\n",
      "Okay, so I need to figure out the features of the Alpine Explorer Tent based on the given context. Let me start by reading the context again carefully.\n",
      "\n",
      "The context says: The Alpine Explorer Tent features a detachable partition to ensure privacy, numerous mesh windows and adjustable vents for ventilation, and a waterproof design. It also includes a built-in gear loft for storing outdoor essentials. In short, it offers a harmonious blend of privacy, comfort, and convenience, making it a second home in nature!\n",
      "\n",
      "Hmm, the question is asking for the features, so I need to list out each feature mentioned here. Let me parse each sentence.\n",
      "\n",
      "First, \"detachable partition to ensure privacy\" – that's one feature: a detachable partition for privacy. Next, \"numerous mesh windows and adjustable vents for ventilation\" – so two parts here: mesh windows and adjustable vents, both for ventilation. Then, \"waterproof design\" – another feature. Also, \"built-in gear loftNoneCPU times: user 393 ms, sys: 61.6 ms, total: 455 ms\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = llm.chat.completions.create(\n",
    "    model=model_path,\n",
    "    messages=[\n",
    "        {\"role\": \"saystem\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    "    stream=True,  # Stream the response\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in response:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if hasattr(delta, \"content\"):\n",
    "        print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff7ee2-d17b-4488-877c-9877f192fca1",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bed0e352-5d6f-459d-b27f-3c9f37536ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'text': '46-square-mile peninsula surrounded by water on three sides. The city is home to some of the most expensive real estate in the country and is known for its steep hills. The hills are a result of the city’s location on the San Andreas Fault, which runs through California. The fault is responsible for the city’s earthquakes, which have caused the hills to form over time.\\n\\nSan Francisco has 43 hills, according to the city’s official website. The city is located on a peninsula that is surrounded by the Pacific Ocean, San Francisco Bay, and Golden Gate. The steepness of the hills is due in part to the city’s location on the San Andreas Fault. Because San Francisco is so hilly, many of its streets are extremely steep. It was built on seven hills, similar to Rome. The city’s highest peak is Mount Davidson, which is located at 927 feet above sea level. The San Andreas Fault runs through the city and is responsible for earthquakes.\\n\\n', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'created': 1740915416, 'model': '/models/deepseek-adapter/DeepSeek-R1-Distill-Qwen-1.5B', 'system_fingerprint': 'b4798-1782cdfe', 'object': 'text_completion', 'usage': {'completion_tokens': 200, 'prompt_tokens': 6, 'total_tokens': 206}, 'id': 'chatcmpl-qrcYbs1BDZ5ZkRdZnC8bKH9ELJMcba9k', 'timings': {'prompt_n': 5, 'prompt_ms': 247.045, 'prompt_per_token_ms': 49.409, 'prompt_per_second': 20.239227671072072, 'predicted_n': 200, 'predicted_ms': 15028.289, 'predicted_per_token_ms': 75.141445, 'predicted_per_second': 13.308234889547304}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "completions_url = os.path.join(endpoint_results.scoring_uri, \"v1/completions\")\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {primary_key}\"}\n",
    "data = {\n",
    "    \"model\": model_path,\n",
    "    \"prompt\": \"San Francisco is a \",\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "response = requests.post(completions_url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d96bd-75da-4c10-923c-edad899fc4d3",
   "metadata": {},
   "source": [
    "### 4.2. LLM latency/throughput simple benchmarking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb105465-85dd-49ee-a45f-403f7d0c9d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "def simple_llm_benchmark(\n",
    "    llm: OpenAI,\n",
    "    messages: list,\n",
    "    model_path: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    num_warmups: int = 1,\n",
    "    num_infers: int = 5,\n",
    "    **params: dict,\n",
    ") -> dict:\n",
    "\n",
    "    print(\"=== Measuring latency ===\")\n",
    "    print(f\"model_path={model_path}, num_infers={num_infers}, params={params}\")\n",
    "\n",
    "    latencies = []\n",
    "    # Warm up\n",
    "    for _ in range(num_warmups):\n",
    "        response = llm.chat.completions.create(\n",
    "            model=model_path,\n",
    "            messages=messages,\n",
    "            **params,\n",
    "        )\n",
    "    print(\"=== Warmup done. Start Benchmarking... ===\")\n",
    "    begin = time.time()\n",
    "    # Timed run\n",
    "    for curr_infer in range(num_infers):\n",
    "        start_time = perf_counter()\n",
    "        if (curr_infer % 5) == 0:\n",
    "            print(f\"Inferring {curr_infer}th...\")\n",
    "        response = llm.chat.completions.create(\n",
    "            model=model_path,\n",
    "            messages=messages,\n",
    "            **params,\n",
    "        )\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute run statistics\n",
    "    duration = end - begin\n",
    "    time_avg_sec = np.mean(latencies)\n",
    "    time_std_sec = np.std(latencies)\n",
    "    time_p95_sec = np.percentile(latencies, 95)\n",
    "    time_p99_sec = np.percentile(latencies, 99)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"duration\": duration,\n",
    "        \"avg_sec\": time_avg_sec,\n",
    "        \"std_sec\": time_std_sec,\n",
    "        \"p95_sec\": time_p95_sec,\n",
    "        \"p99_sec\": time_p99_sec,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8827fb45-c7ab-4aa8-b9f4-1c4e610d695a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Measuring latency ===\n",
      "model_path=/models/deepseek-adapter/DeepSeek-R1-Distill-Qwen-1.5B, num_infers=10, params={'max_tokens': 100, 'temperature': 0.5}\n",
      "=== Warmup done. Start Benchmarking... ===\n",
      "Inferring 0th...\n",
      "Inferring 5th...\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "params = {\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "model_path = \"/models/DeepSeek-R1-UD-IQ1_M\"\n",
    "\n",
    "metrics = simple_llm_benchmark(\n",
    "    llm, messages, model_path=model_path, num_warmups=1, num_infers=10, **params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa138375-6751-4502-91ea-29b3b3725a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_sec': 7.823659540410153,\n",
      " 'duration': 78.23661303520203,\n",
      " 'p95_sec': 7.972206991264829,\n",
      " 'p99_sec': 7.999277103060158,\n",
      " 'std_sec': 0.15908935995443393}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834a237-e751-446a-ac21-7272c29b0c2c",
   "metadata": {},
   "source": [
    "## Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc72663d-d773-435b-871f-cc51b1e51763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f417cd79390>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................."
     ]
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_delete(azure_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
